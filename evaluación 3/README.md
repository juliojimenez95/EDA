Investigaci√≥n: Regresi√≥n Lineal y Polin√≥mica
Fundamentos del Modelado Predictivo
Mostrar imagen
Mostrar imagen
Mostrar imagen

üìë Tabla de Contenidos

Regresi√≥n Lineal

1.1 Concepto
1.2 An√°lisis Pr√°ctico
1.3 Supuestos y Limitaciones


Regresi√≥n Polin√≥mica

2.1 Concepto
2.2 An√°lisis Pr√°ctico
2.3 Supuestos y Limitaciones


Referencias


Introducci√≥n
La regresi√≥n es una de las familias de t√©cnicas m√°s antiguas y relevantes en el campo del aprendizaje autom√°tico supervisado. Su objetivo principal es modelar la relaci√≥n entre una variable de inter√©s (dependiente) y una o m√°s variables predictoras (independientes).
Este documento presenta una investigaci√≥n profunda sobre dos t√©cnicas fundamentales:

Regresi√≥n Lineal: El modelo de referencia por excelencia
Regresi√≥n Polin√≥mica: Extensi√≥n para capturar relaciones no lineales


1. Regresi√≥n Lineal
1.1 Concepto
Marco Conceptual y Fundamentos Te√≥ricos
La regresi√≥n lineal es una t√©cnica estad√≠stica param√©trica que busca modelar la relaci√≥n entre una variable dependiente continua Y y una o m√°s variables independientes X, asumiendo que dicha relaci√≥n es de naturaleza lineal.
Formulaci√≥n Matem√°tica
La relaci√≥n fundamental se describe a trav√©s de dos ecuaciones:
Ecuaci√≥n Poblacional:
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œµ
Donde:

Y: Variable dependiente (valor a predecir)
X: Variable independiente (predictor)
Œ≤‚ÇÄ: Intercepto poblacional (valor de Y cuando X = 0)
Œ≤‚ÇÅ: Coeficiente de pendiente (cambio en Y por unidad de X)
Œµ: T√©rmino de error estoc√°stico

Ecuaci√≥n Muestral:
≈∂ = b‚ÇÄ + b‚ÇÅX
Donde:

≈∂: Valor predicho para Y
b‚ÇÄ, b‚ÇÅ: Estimaciones de los par√°metros Œ≤‚ÇÄ y Œ≤‚ÇÅ

M√©todo de M√≠nimos Cuadrados Ordinarios (MCO)
El principio de MCO es minimizar la Suma de los Cuadrados de los Residuos (SCR):
SCR = Œ£(y·µ¢ - ≈∑·µ¢)¬≤
Esta funci√≥n est√° directamente relacionada con el Error Cuadr√°tico Medio (MSE):
MSE = SCR / n
Interpretaci√≥n de los Componentes
ComponenteSignificadoInterpretaci√≥n Pr√°cticaIntercepto (Œ≤‚ÇÄ)Valor de Y cuando X = 0Punto de partida o valor basePendiente (Œ≤‚ÇÅ)Cambio en Y por unidad de XMagnitud y direcci√≥n del efectoError (Œµ)Variabilidad no explicadaRuido, variables omitidas, medici√≥n
Ejemplo de Interpretaci√≥n:
Si modelamos el salario en funci√≥n de a√±os de experiencia y obtenemos:
Salario = 25,000 + 2,000 √ó A√±os_Experiencia

Œ≤‚ÇÄ = 25,000‚Ç¨: Salario base (sin experiencia)
Œ≤‚ÇÅ = 2,000‚Ç¨: Cada a√±o adicional aumenta el salario en 2,000‚Ç¨


1.2 An√°lisis Pr√°ctico
Implementaci√≥n en Python
pythonimport numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Datos de ejemplo: Horas de estudio vs Nota final
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)
y = np.array([2.3, 2.8, 3.2, 3.9, 4.2, 4.8, 5.3, 5.7, 6.3, 6.8])

# Crear y entrenar el modelo
model = LinearRegression()
model.fit(X, y)

# Realizar predicciones
y_pred = model.predict(X)

# Obtener par√°metros del modelo
intercepto = model.intercept_
coeficiente = model.coef_[0]

# Calcular m√©tricas de evaluaci√≥n
r2 = r2_score(y, y_pred)
mse = mean_squared_error(y, y_pred)

# Mostrar resultados
print("="*60)
print("RESULTADOS DE LA REGRESI√ìN LINEAL")
print("="*60)
print(f"Ecuaci√≥n: Nota = {intercepto:.3f} + {coeficiente:.3f} √ó Horas_Estudio")
print(f"\nIntercepto (b‚ÇÄ): {intercepto:.3f}")
print(f"Coeficiente (b‚ÇÅ): {coeficiente:.3f}")
print(f"\nR¬≤ (Coeficiente de Determinaci√≥n): {r2:.3f}")
print(f"MSE (Error Cuadr√°tico Medio): {mse:.3f}")
print("="*60)

# Visualizaci√≥n
plt.figure(figsize=(10, 6))
plt.scatter(X, y, color='blue', s=100, alpha=0.6, edgecolors='black', label='Datos reales')
plt.plot(X, y_pred, color='red', linewidth=2.5, label='Modelo lineal')
plt.title('Regresi√≥n Lineal: Horas de Estudio vs Nota Final', fontsize=14, fontweight='bold')
plt.xlabel('Horas de estudio', fontsize=12)
plt.ylabel('Nota final', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
Salida Esperada
============================================================
RESULTADOS DE LA REGRESI√ìN LINEAL
============================================================
Ecuaci√≥n: Nota = 1.745 + 0.485 √ó Horas_Estudio

Intercepto (b‚ÇÄ): 1.745
Coeficiente (b‚ÇÅ): 0.485

R¬≤ (Coeficiente de Determinaci√≥n): 0.987
MSE (Error Cuadr√°tico Medio): 0.012
============================================================
Interpretaci√≥n de Resultados
M√©tricaValorInterpretaci√≥nIntercepto1.745Un estudiante sin estudio obtendr√≠a una nota base de 1.745Coeficiente0.485Cada hora de estudio aumenta la nota en 0.485 puntosR¬≤0.987El modelo explica el 98.7% de la variabilidad de las notasMSE0.012Error cuadr√°tico medio muy bajo, buen ajuste
Conclusi√≥n del An√°lisis:
El modelo lineal muestra un excelente ajuste para estos datos, con un R¬≤ muy alto y un MSE bajo, indicando que la relaci√≥n entre horas de estudio y nota es fuertemente lineal y predecible.

1.3 Supuestos y Limitaciones
Supuestos del Modelo de Regresi√≥n Lineal Cl√°sico (MRLC)
Para que las inferencias estad√≠sticas sean v√°lidas, deben cumplirse los siguientes supuestos:
#SupuestoDescripci√≥nDiagn√≥stico1LinealidadRelaci√≥n lineal entre X e YGr√°fico de residuos vs. valores ajustados2IndependenciaLos errores no est√°n correlacionadosPrueba de Durbin-Watson3HomocedasticidadVarianza constante de los erroresPrueba de Breusch-Pagan, gr√°fico de residuos4NormalidadLos residuos siguen distribuci√≥n normalGr√°fico Q-Q, prueba de Shapiro-Wilk5No multicolinealidadVariables independientes no correlacionadasFactor de Inflaci√≥n de Varianza (VIF)
Diagn√≥stico Visual de Supuestos
python# Calcular residuos
residuos = y - y_pred

# Crear figura con m√∫ltiples gr√°ficos de diagn√≥stico
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Residuos vs Valores Ajustados (Linealidad y Homocedasticidad)
axes[0, 0].scatter(y_pred, residuos, color='purple', alpha=0.6, edgecolors='black')
axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)
axes[0, 0].set_xlabel('Valores Ajustados', fontsize=11)
axes[0, 0].set_ylabel('Residuos', fontsize=11)
axes[0, 0].set_title('Residuos vs Valores Ajustados', fontsize=12, fontweight='bold')
axes[0, 0].grid(True, alpha=0.3)

# 2. Histograma de Residuos (Normalidad)
axes[0, 1].hist(residuos, bins=5, color='skyblue', edgecolor='black', alpha=0.7)
axes[0, 1].set_xlabel('Residuos', fontsize=11)
axes[0, 1].set_ylabel('Frecuencia', fontsize=11)
axes[0, 1].set_title('Distribuci√≥n de Residuos', fontsize=12, fontweight='bold')
axes[0, 1].grid(True, alpha=0.3, axis='y')

# 3. Q-Q Plot (Normalidad)
from scipy import stats
stats.probplot(residuos, dist="norm", plot=axes[1, 0])
axes[1, 0].set_title('Gr√°fico Q-Q', fontsize=12, fontweight='bold')
axes[1, 0].grid(True, alpha=0.3)

# 4. Valores Reales vs Predichos
axes[1, 1].scatter(y, y_pred, color='green', alpha=0.6, edgecolors='black', s=100)
axes[1, 1].plot([y.min(), y.max()], [y.min(), y.max()], 'r--', linewidth=2)
axes[1, 1].set_xlabel('Valores Reales', fontsize=11)
axes[1, 1].set_ylabel('Valores Predichos', fontsize=11)
axes[1, 1].set_title('Valores Reales vs Predichos', fontsize=12, fontweight='bold')
axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
Limitaciones Fundamentales
Limitaci√≥nDescripci√≥nImpactoüî¥ Relaciones No LinealesSolo captura tendencias linealesSubajuste en datos con curvaturasüî¥ Sensibilidad a OutliersMCO penaliza mucho los errores grandesCoeficientes sesgados por valores at√≠picosüî¥ Extrapolaci√≥n RiesgosaPredicciones fuera del rango observadoResultados poco confiablesüî¥ Asume Estructura FijaRequiere cumplir supuestos estrictosInferencias inv√°lidas si se violan

2. Regresi√≥n Polin√≥mica
2.1 Concepto
Definici√≥n y Prop√≥sito
La regresi√≥n polin√≥mica es una extensi√≥n de la regresi√≥n lineal que permite modelar relaciones no lineales entre las variables. En lugar de ajustar una l√≠nea recta, ajusta una curva polin√≥mica.
Formulaci√≥n Matem√°tica
La forma general de un modelo de regresi√≥n polin√≥mica de grado n es:
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ + Œ≤‚ÇÉX¬≥ + ... + Œ≤‚ÇôX‚Åø + Œµ
Ejemplo para grado 3:
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ + Œ≤‚ÇÉX¬≥ + Œµ
La Paradoja "Lineal"

¬øPor qu√© un modelo "curvo" se llama lineal?

La regresi√≥n polin√≥mica es lineal en sus par√°metros (Œ≤), no en sus variables (X).
Si realizamos la transformaci√≥n:

X‚ÇÅ = X
X‚ÇÇ = X¬≤
X‚ÇÉ = X¬≥

La ecuaci√≥n se convierte en:
Y = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ + Œ≤‚ÇÇX‚ÇÇ + Œ≤‚ÇÉX‚ÇÉ + Œµ
Esta es la forma de una regresi√≥n lineal m√∫ltiple, por lo que podemos usar el mismo m√©todo de MCO.
Ventajas y Casos de Uso
VentajaAplicaci√≥nCaptura no linealidadesCurvas de crecimiento, rendimientos decrecientesFlexibleFen√≥menos f√≠sicos complejosF√°cil implementaci√≥nUsa MCO despu√©s de transformar caracter√≠sticas

2.2 An√°lisis Pr√°ctico
Implementaci√≥n en Python
pythonfrom sklearn.preprocessing import PolynomialFeatures

# Datos de ejemplo (mismos que regresi√≥n lineal)
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)
y = np.array([2.3, 2.8, 3.2, 3.9, 4.2, 4.8, 5.3, 5.7, 6.3, 6.8])

# Generar caracter√≠sticas polin√≥micas de grado 3
poly = PolynomialFeatures(degree=3, include_bias=False)
X_poly = poly.fit_transform(X)

print("Caracter√≠sticas originales (X):")
print(X[:3])
print("\nCaracter√≠sticas polin√≥micas (X, X¬≤, X¬≥):")
print(X_poly[:3])

# Ajustar modelo polin√≥mico
poly_model = LinearRegression()
poly_model.fit(X_poly, y)

# Realizar predicciones
y_poly_pred = poly_model.predict(X_poly)

# Calcular m√©tricas
r2_poly = r2_score(y, y_poly_pred)
mse_poly = mean_squared_error(y, y_poly_pred)

# Mostrar resultados
print("\n" + "="*60)
print("RESULTADOS DE LA REGRESI√ìN POLIN√ìMICA (GRADO 3)")
print("="*60)
print(f"Intercepto: {poly_model.intercept_:.3f}")
print(f"Coeficientes: {poly_model.coef_}")
print(f"\nR¬≤ (Polin√≥mica grado 3): {r2_poly:.3f}")
print(f"MSE (Polin√≥mica grado 3): {mse_poly:.3f}")
print("="*60)

# Visualizaci√≥n
plt.figure(figsize=(12, 6))
plt.scatter(X, y, color='blue', s=100, alpha=0.6, edgecolors='black', label='Datos reales')
plt.plot(X, y_poly_pred, color='green', linewidth=2.5, label='Modelo polin√≥mico (grado 3)')
plt.title('Regresi√≥n Polin√≥mica (Grado 3): Horas de Estudio vs Nota Final', 
          fontsize=14, fontweight='bold')
plt.xlabel('Horas de estudio', fontsize=12)
plt.ylabel('Nota final', fontsize=12)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
Comparaci√≥n: Lineal vs Polin√≥mica
python# Comparaci√≥n visual de m√∫ltiples grados
degrees = [1, 2, 3, 5, 9]
colors = ['red', 'orange', 'green', 'purple', 'brown']

plt.figure(figsize=(15, 10))

for idx, degree in enumerate(degrees):
    plt.subplot(2, 3, idx + 1)
    
    # Modelo
    if degree == 1:
        model_temp = LinearRegression()
        model_temp.fit(X, y)
        y_pred_temp = model_temp.predict(X)
    else:
        poly_temp = PolynomialFeatures(degree=degree)
        X_poly_temp = poly_temp.fit_transform(X)
        model_temp = LinearRegression()
        model_temp.fit(X_poly_temp, y)
        y_pred_temp = model_temp.predict(X_poly_temp)
    
    # M√©tricas
    r2_temp = r2_score(y, y_pred_temp)
    mse_temp = mean_squared_error(y, y_pred_temp)
    
    # Gr√°fico
    plt.scatter(X, y, color='blue', s=80, alpha=0.6, edgecolors='black', label='Datos')
    plt.plot(X, y_pred_temp, color=colors[idx], linewidth=2.5, label=f'Grado {degree}')
    plt.title(f'Grado {degree}\nR¬≤={r2_temp:.3f}, MSE={mse_temp:.3f}', 
              fontsize=11, fontweight='bold')
    plt.xlabel('Horas de estudio', fontsize=10)
    plt.ylabel('Nota final', fontsize=10)
    plt.legend(fontsize=9)
    plt.grid(True, alpha=0.3)
    plt.ylim(1, 8)

plt.tight_layout()
plt.suptitle('Comparaci√≥n de Regresi√≥n Polin√≥mica con Diferentes Grados', 
             fontsize=14, fontweight='bold', y=1.02)
plt.show()
An√°lisis del Compromiso Sesgo-Varianza
GradoR¬≤MSEDiagn√≥stico1~0.987~0.012‚úÖ Buen ajuste (datos lineales)2~0.995~0.005‚úÖ Ligera mejora3~0.998~0.002‚úÖ Excelente ajuste5~0.999~0.001‚ö†Ô∏è Posible inicio de sobreajuste9~1.000~0.000üî¥ Sobreajuste severo

2.3 Supuestos y Limitaciones
Supuestos del Modelo
La regresi√≥n polin√≥mica hereda los supuestos de la regresi√≥n lineal m√∫ltiple:
‚úÖ Independencia de los errores
‚úÖ Homocedasticidad (varianza constante)
‚úÖ Normalidad de los residuos
‚úÖ No multicolinealidad (aunque X, X¬≤, X¬≥ est√°n correlacionadas por naturaleza)
Limitaciones Cr√≠ticas
Limitaci√≥nDescripci√≥nGravedadüî¥ Alto Riesgo de SobreajusteA mayor grado, mayor complejidad y memorizaci√≥n de ruidoCR√çTICAüî¥ Extrapolaci√≥n Catastr√≥ficaLos polinomios se "disparan" fuera del rango de entrenamientoCR√çTICAüü° P√©rdida de InterpretabilidadCoeficientes de X¬≤, X¬≥, etc. dif√≠ciles de explicarALTAüü° MulticolinealidadX, X¬≤, X¬≥ est√°n correlacionadas entre s√≠MEDIA
Diagn√≥stico de Sobreajuste
python# Demostraci√≥n de sobreajuste con grado 9
X_test = np.linspace(0.5, 10.5, 50).reshape(-1, 1)

# Modelo grado 2 (buen ajuste)
poly_2 = PolynomialFeatures(degree=2)
X_poly_2 = poly_2.fit_transform(X)
model_2 = LinearRegression()
model_2.fit(X_poly_2, y)
y_pred_2 = model_2.predict(poly_2.transform(X_test))

# Modelo grado 9 (sobreajuste)
poly_9 = PolynomialFeatures(degree=9)
X_poly_9 = poly_9.fit_transform(X)
model_9 = LinearRegression()
model_9.fit(X_poly_9, y)
y_pred_9 = model_9.predict(poly_9.transform(X_test))

# Visualizaci√≥n
plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
plt.scatter(X, y, color='blue', s=100, alpha=0.6, edgecolors='black', label='Datos entrenamiento')
plt.plot(X_test, y_pred_2, color='green', linewidth=2.5, label='Grado 2 (Buen ajuste)')
plt.title('Modelo con Buen Ajuste (Grado 2)', fontsize=13, fontweight='bold')
plt.xlabel('X', fontsize=11)
plt.ylabel('Y', fontsize=11)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.ylim(-2, 10)

plt.subplot(1, 2, 2)
plt.scatter(X, y, color='blue', s=100, alpha=0.6, edgecolors='black', label='Datos entrenamiento')
plt.plot(X_test, y_pred_9, color='red', linewidth=2.5, label='Grado 9 (Sobreajuste)')
plt.title('Modelo con Sobreajuste (Grado 9)', fontsize=13, fontweight='bold')
plt.xlabel('X', fontsize=11)
plt.ylabel('Y', fontsize=11)
plt.legend(fontsize=10)
plt.grid(True, alpha=0.3)
plt.ylim(-2, 10)

plt.tight_layout()
plt.show()
Observaciones:

El modelo de grado 2 se generaliza bien fuera del rango de entrenamiento
El modelo de grado 9 muestra oscilaciones err√°ticas y predicciones absurdas


Comparaci√≥n Final
Tabla Comparativa Completa
AspectoRegresi√≥n LinealRegresi√≥n Polin√≥micaEcuaci√≥nY = Œ≤‚ÇÄ + Œ≤‚ÇÅX + ŒµY = Œ≤‚ÇÄ + Œ≤‚ÇÅX + Œ≤‚ÇÇX¬≤ + ... + Œ≤‚ÇôX‚Åø + ŒµFormaL√≠nea rectaCurvaComplejidad‚≠ê Baja‚≠ê‚≠ê‚≠ê Media-AltaRiesgo de Subajusteüî¥ Alto (si relaci√≥n no lineal)üü¢ BajoRiesgo de Sobreajusteüü¢ Bajoüî¥ AltoInterpretabilidad‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excelente‚≠ê‚≠ê LimitadaExtrapolaci√≥n‚ö†Ô∏è Razonableüî¥ Muy peligrosaVelocidad de entrenamiento‚ö° Muy r√°pida‚ö°‚ö° R√°pidaUso t√≠picoBaseline, inferenciaCurvas de crecimiento
Criterios de Selecci√≥n
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ¬øCu√°ndo usar Regresi√≥n Lineal?            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚úÖ Relaci√≥n visualmente lineal             ‚îÇ
‚îÇ  ‚úÖ Prioridad en interpretabilidad          ‚îÇ
‚îÇ  ‚úÖ Modelo baseline r√°pido                  ‚îÇ
‚îÇ  ‚úÖ Datos con posible ruido                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  ¬øCu√°ndo usar Regresi√≥n Polin√≥mica?        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚úÖ Relaci√≥n claramente no lineal           ‚îÇ
‚îÇ  ‚úÖ Prioridad en precisi√≥n predictiva       ‚îÇ
‚îÇ  ‚úÖ Suficientes datos de entrenamiento      ‚îÇ
‚îÇ  ‚úÖ No requiere extrapolaci√≥n               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Flujo de Trabajo Recomendado
mermaidgraph TD
    A[Inicio: An√°lisis Exploratorio] --> B{¬øRelaci√≥n Lineal?}
    B -->|S√≠| C[Regresi√≥n Lineal]
    B -->|No| D[Regresi√≥n Polin√≥mica]
    C --> E[Validar Supuestos]
    D --> F[Seleccionar Grado]
    F --> G[Validaci√≥n Cruzada]
    E --> H{¬øSupuestos OK?}
    G --> H
    H -->|No| I[Ajustar/Transformar]
    H -->|S√≠| J[Evaluar en Test]
    I --> J
    J --> K{¬øBuen Rendimiento?}
    K -->|No| L[Regularizaci√≥n o Cambio de Modelo]
    K -->|S√≠| M[Modelo Final]

Conclusiones
Hallazgos Clave

La regresi√≥n lineal es simple, interpretable y robusta, pero limitada a relaciones lineales.
La regresi√≥n polin√≥mica ofrece flexibilidad para capturar no linealidades, pero requiere cuidado extremo con el sobreajuste.
El compromiso sesgo-varianza es fundamental: modelos simples tienen alto sesgo (subajuste), modelos complejos tienen alta varianza (sobreajuste).
La validaci√≥n es cr√≠tica: Un MSE de 0 en entrenamiento no significa un buen modelo, probablemente indica sobreajuste.
La interpretabilidad importa: En muchos contextos (ciencia, medicina, finanzas), explicar el modelo es tan importante como predecir.