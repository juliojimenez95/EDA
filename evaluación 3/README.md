# Regresi√≥n Lineal y Polin√≥mica

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![scikit-learn](https://img.shields.io/badge/scikit--learn-latest-orange.svg)
![NumPy](https://img.shields.io/badge/NumPy-latest-013243.svg)

Investigaci√≥n sobre los fundamentos del modelado predictivo utilizando t√©cnicas de regresi√≥n lineal y polin√≥mica.

---

## üìë Tabla de Contenidos

- [1. Regresi√≥n Lineal](#1-regresi√≥n-lineal)
  - [1.1 Concepto General](#11-concepto-general)
  - [1.2 An√°lisis Pr√°ctico](#12-an√°lisis-pr√°ctico)
  - [1.3 Supuestos y Limitaciones](#13-supuestos-y-limitaciones)
- [2. Regresi√≥n Polin√≥mica](#2-regresi√≥n-polin√≥mica)
  - [2.1 Concepto General](#21-concepto-general)
  - [2.2 An√°lisis Pr√°ctico](#22-an√°lisis-pr√°ctico)
  - [2.3 Supuestos y Limitaciones](#23-supuestos-y-limitaciones)
- [Comparaci√≥n](#comparaci√≥n)
- [Instalaci√≥n](#instalaci√≥n)
- [Referencias](#referencias)

---

## 1. Regresi√≥n Lineal

### 1.1 Concepto General

La **regresi√≥n lineal** es un m√©todo estad√≠stico que modela la relaci√≥n entre una variable dependiente **Y** y una o m√°s variables independientes **X‚ÇÅ, X‚ÇÇ, ..., X‚Çö** suponiendo una relaci√≥n lineal.

**Modelo general:**
Y·µ¢ = Œ≤‚ÇÄ + Œ≤‚ÇÅX‚ÇÅ·µ¢ + Œ≤‚ÇÇX‚ÇÇ·µ¢ + ... + Œ≤‚ÇöX‚Çö·µ¢ + Œµ·µ¢

**Donde:**
- **Œ≤‚ÇÄ**: Intercepto o t√©rmino independiente
- **Œ≤‚±º**: Pendiente o coeficiente de regresi√≥n
- **Œµ·µ¢**: Error aleatorio del modelo (media cero, varianza constante œÉ¬≤)

**Objetivo:** Estimar los par√°metros **Œ≤** minimizando la suma de errores cuadr√°ticos (M√≠nimos Cuadrados Ordinarios, MCO):
min Œ≤: Œ£(Y·µ¢ - ≈∂·µ¢)¬≤ = min Œ≤: (Y - XŒ≤)·µÄ(Y - XŒ≤)

**Soluci√≥n matricial:**
Œ≤ÃÇ = (X·µÄX)‚Åª¬πX·µÄY

donde **X** es la matriz de dise√±o que incluye una columna de unos para el intercepto.

---

### 1.2 An√°lisis Pr√°ctico

**Ejemplo:** Predecir la nota de un estudiante en funci√≥n de las horas de estudio.
```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Datos de ejemplo
X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)
y = np.array([2.3, 2.8, 3.2, 3.9, 4.2, 4.8, 5.3, 5.7, 6.3, 6.8])

# Crear y entrenar modelo
model = LinearRegression()
model.fit(X, y)

# Predicciones
y_pred = model.predict(X)

# M√©tricas
r2 = r2_score(y, y_pred)
mse = mean_squared_error(y, y_pred)

print(f"Ecuaci√≥n: y = {model.intercept_:.2f} + {model.coef_[0]:.2f}x")
print(f"R¬≤: {r2:.3f}, MSE: {mse:.3f}")

# Visualizaci√≥n
plt.scatter(X, y, color='blue', label='Datos reales')
plt.plot(X, y_pred, color='red', label='Recta ajustada')
plt.title('Regresi√≥n Lineal Simple')
plt.xlabel('Horas de estudio')
plt.ylabel('Nota final')
plt.legend()
plt.show()
Interpretaci√≥n:
El modelo ajusta una l√≠nea recta a los datos. Un R¬≤ alto indica una buena proporci√≥n de varianza explicada por las horas de estudio.

1.3 Supuestos y Limitaciones
Supuestos Fundamentales
SupuestoDescripci√≥nLinealidadLa relaci√≥n entre X e Y es linealIndependenciaLos residuos no est√°n correlacionadosHomocedasticidadLa varianza de los errores es constanteNormalidadLos residuos siguen una distribuci√≥n normalNo multicolinealidadVariables independientes no altamente correlacionadas
Limitaciones

No modela relaciones no lineales
Sensible a valores at√≠picos (outliers)
Depende del cumplimiento estricto de supuestos para inferencias v√°lidas


2. Regresi√≥n Polin√≥mica
2.1 Concepto General
La regresi√≥n polin√≥mica extiende el modelo lineal al incluir potencias de las variables independientes:
Y·µ¢ = Œ≤‚ÇÄ + Œ≤‚ÇÅX·µ¢ + Œ≤‚ÇÇX·µ¢¬≤ + ... + Œ≤‚ÇêX·µ¢·µà + Œµ·µ¢
Caracter√≠sticas clave:

Aunque el modelo es no lineal respecto a X, sigue siendo lineal en los par√°metros Œ≤
Permite capturar relaciones curvil√≠neas entre variables
Se adapta mejor a tendencias no lineales

Matriz de dise√±o:
X = [ 1  X‚ÇÅ  X‚ÇÅ¬≤  ...  X‚ÇÅ·µà ]
    [ 1  X‚ÇÇ  X‚ÇÇ¬≤  ...  X‚ÇÇ·µà ]
    [ ‚ãÆ   ‚ãÆ   ‚ãÆ   ‚ã±    ‚ãÆ  ]
    [ 1  X‚Çô  X‚Çô¬≤  ...  X‚Çô·µà ]

2.2 An√°lisis Pr√°ctico
pythonfrom sklearn.preprocessing import PolynomialFeatures

# Generar caracter√≠sticas polin√≥micas de grado 3
poly = PolynomialFeatures(degree=3)
X_poly = poly.fit_transform(X)

# Ajustar modelo polin√≥mico
poly_model = LinearRegression()
poly_model.fit(X_poly, y)

y_poly_pred = poly_model.predict(X_poly)

# M√©tricas
r2_poly = r2_score(y, y_poly_pred)
mse_poly = mean_squared_error(y, y_poly_pred)

print(f"R¬≤ (Polin√≥mica grado 3): {r2_poly:.3f}, MSE: {mse_poly:.3f}")

# Visualizaci√≥n
plt.scatter(X, y, color='blue', label='Datos reales')
plt.plot(X, y_poly_pred, color='green', label='Modelo polin√≥mico (grado 3)')
plt.title('Regresi√≥n Polin√≥mica')
plt.xlabel('Horas de estudio')
plt.ylabel('Nota final')
plt.legend()
plt.show()
Interpretaci√≥n:

A mayor grado del polinomio, mejor ajuste a los datos (menor error de entrenamiento)
Grado muy alto puede causar Sobreajuste (overfitting) y p√©rdida de capacidad de generalizaci√≥n


2.3 Supuestos y Limitaciones
Supuestos

Los errores siguen siendo independientes, homoced√°sticos y normales
El modelo sigue siendo lineal en los coeficientes Œ≤

Limitaciones
Limitaci√≥nImpactoAlto riesgo de sobreajusteCon grados grandes, el modelo memoriza ruidoMala extrapolaci√≥nPredicciones inestables fuera del rango de X observadoDificultad de interpretaci√≥nCoeficientes complejos al aumentar el grado

Comparaci√≥n
Caracter√≠sticaRegresi√≥n LinealRegresi√≥n Polin√≥micaTipo de relaci√≥nLinealNo lineal (curvil√≠nea)Complejidad del modeloBajaMedia‚ÄìAltaInterpretabilidadAltaMedia o bajaRiesgo de sobreajusteBajoAltoGeneralizaci√≥nBuenaVariableAplicacionesTendencias lineales, pron√≥sticos simplesModelos con curvaturas, fen√≥menos f√≠sicos/biol√≥gicos
